```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Top 10 AI Research Papers - 2025-08-12</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 30px;
        }
        .paper {
            background-color: #f9f9f9;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .title {
            font-size: 1.2em;
            font-weight: bold;
            color: #2980b9;
            margin-bottom: 10px;
        }
        .title a {
            color: inherit;
            text-decoration: none;
        }
        .title a:hover {
            text-decoration: underline;
        }
        .authors {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 10px;
        }
        .abstract {
            margin-bottom: 10px;
        }
        .link {
            color: #27ae60;
            text-decoration: none;
        }
        .link:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>Top 10 AI Research Papers published on 2025-08-12</h1>

    <div class="paper">
        <div class="title"><a href="http://arxiv.org/abs/2508.09389v1" target="_blank">ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs</a></div>
        <div class="authors">Eray Eren, Qingju Liu, Hyeongwoo Kim, Pablo Garrido, Abeer Alwan</div>
        <div class="abstract">Prosody conveys rich emotional and semantic information of the speech signal as well as individual idiosyncrasies. We propose a stand-alone model that maps text-to-prosodic features such as F0 and energy and can be used in downstream tasks such as TTS. The ProMode encoder takes as input acoustic features and time-aligned textual content, both are partially masked, and obtains a fixed-length latent prosodic embedding.</div>
        <a class="link" href="http://arxiv.org/abs/2508.09389v1" target="_blank">View Paper</a>
    </div>

    <div class="paper">
        <div class="title"><a href="http://arxiv.org/abs/2508.09378v1" target="_blank">APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification</a></div>
        <div class="authors">Artem Chernodub, Aman Saini, Yejin Huh, Vivek Kulkarni, Vipul Raheja</div>
        <div class="abstract">Recent advancements in large language models (LLMs) have enabled a wide range of natural language processing (NLP) tasks to be performed through simple prompt-based interactions. Consequently, several approaches have been proposed to engineer prompts that most effectively enable LLMs to perform a given task. In settings with a well-defined metric to optimize model performance, automatic prompt optimization (APO) methods have been developed to refine a seed prompt.</div>
        <a class="link" href="http://arxiv.org/abs/2508.09378v1" target="_blank">View Paper</a>
    </div>

    <div class="paper">
        <div class="title"><a href="http://arxiv.org/abs/2508.09350v1" target="_blank">Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling</a></div>
        <div class="authors">Ju-Chieh Chou, Jiawei Zhou, Karen Livescu</div>
        <div class="abstract">Textless spoken language models (SLMs) are generative models of speech that do not rely on text supervision. Most textless SLMs learn to predict the next semantic token, a discrete representation of linguistic content, and rely on a separate vocoder to add acoustic information to the generated speech. Such models have no access to acoustic context and no built-in control over acoustic details.</div>
        <a class="link" href="http://arxiv.org/abs/2508.09350v1" target="_blank">View Paper</a>
    </div>

    <div class="paper">
        <div class="title"><a href="http://arxiv.org/abs/2508.09324v1" target="_blank">TEN: Table Explicitization, Neurosymbolically</a></div>
        <div class="authors">Nikita Mehrotra, Aayush Kumar, Sumit Gulwani, Arjun Radhakrishna, Ashish Tiwari</div>
        <div class="abstract">We present a neurosymbolic approach, TEN, for extracting tabular data from semistructured input text. This task is particularly challenging for text input that does not use special delimiters consistently to separate columns and rows. Purely neural approaches perform poorly due to hallucinations and their inability to enforce hard constraints. TEN uses Structural Decomposition prompting - a specialized chain-of-thought prompting approach.</div>
        <a class="link" href="http://arxiv.org/abs/2508.09324v1" target="_blank">View Paper</a>
    </div>

    <div class="paper">
        <div class="title"><a href="" target="_blank">LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback</a></div>
        <div class="authors">Chen Xu, Zhenyu Lv, Tian Lan, Xianyang Wang, Luyao Ji, Leyang Cui, Minqiang Yang, Jian Shen, Qunxi Dong, Xiuling Liu, Juan Wang, Bin Hu</div>
        <div class="abstract">Although large language models (LLMs) hold significant promise in psychotherapy, their direct application in patient-facing scenarios raises ethical and safety concerns. Therefore, this work shifts towards developing an LLM as a supervisor to train real therapists. In addition to the privacy of clinical therapist training data, a fundamental contradiction complicates the training of therapeutic behaviors.</div>
        <a class="link" href="" target="_blank">View Paper</a>
    </div>
</body>
</html>
```